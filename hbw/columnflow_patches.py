# coding: utf-8

"""
Collection of patches of underlying columnflow tasks.
"""

import getpass
import inspect

import law
import luigi
from columnflow.util import memoize
from columnflow.tasks.framework.base import AnalysisTask
from columnflow.tasks.selection import SelectEvents
from columnflow.tasks.cutflow import CreateCutflowHistograms
from columnflow.tasks.reduction import ReduceEvents
from columnflow.tasks.production import ProduceColumns
from columnflow.tasks.histograms import CreateHistograms
from columnflow.tasks.ml import MLTraining, PrepareMLEvents, MLEvaluation
from columnflow.columnar_util import TaskArrayFunction

logger = law.logger.get_logger(__name__)


@memoize
def patch_live_task_id():
    """
    In c/f, there are some workflows that produce one plot per category/variable combination, but
    the hash generated when running --fetch-output includes the parameters passed to the workflow
    (e.g. variables, categories, processes, etc.). This means that the hash changes when the
    workflow is re-run with different parameters, leading to potentially inconsistent output.
    This patch rewrites the `live_task_id` property of the BaseTask class to only include the task family
    and the parameters that are relevant for the task id, excluding the parameters that change
    frequently (e.g. variables, categories, processes, etc.).

    TODO: we might want to change this patch to not directly modify the "live_task_id" property,
    but instead replace the function that generates the task id string in fetch_output_task (law/tasks/interative.py).
    """
    from law.task.base import BaseTask
    import six
    from law.util import patch_object
    getfullargspec = inspect.getfullargspec if six.PY3 else inspect.getargspec

    remove_from_live_task_id = {
        "datasets", "processes", "known_shifts", "shift_sources",
        "variables", "categories",
    }

    @property
    def live_task_id(self):
        """
        The task id depends on the task family and parameters, and is generated by luigi once in the
        constructor. As the latter may change, this property returns to the id with the current set
        of parameters.
        """
        # create a temporary dictionary of param_kwargs that is patched for the duration of the
        # call to create the string representation of the parameters
        param_kwargs = {
            attr: getattr(self, attr)
            for attr in self.param_kwargs
            # NOTE: this is the only change compared to the original code
            if attr not in remove_from_live_task_id
        }
        # only_public was introduced in luigi 2.8.0, so check if that arg exists
        str_params_kwargs = {"only_significant": True}
        if "only_public" in getfullargspec(self.to_str_params).args:
            str_params_kwargs["only_public"] = True
        with patch_object(self, "param_kwargs", param_kwargs):
            str_params = self.to_str_params(**str_params_kwargs)

        # create the task id
        task_id = luigi.task.task_id_str(self.get_task_family(), str_params)
        task_id = task_id
        # logger.info(f"live_task_id: {task_id}")
        return task_id

    # patch the BaseTask class with the live_task_id property
    logger.info("patching live_task_id property of BaseTask")
    BaseTask.live_task_id = live_task_id


@memoize
def patch_pilots():
    from columnflow.tasks.histograms import MergeShiftedHistograms
    from columnflow.tasks.framework.remote import RemoteWorkflow
    from columnflow.tasks.framework.inference import SerializeInferenceModelBase

    def SerializeInferenceModelBase_workflow_requires(self):
        # cannot call super() here; hopefully we only need the workflow_requires from RemoteWorkflow
        reqs = RemoteWorkflow.workflow_requires(self)
        if not self.pilot:
            reqs["merged_hists"] = hist_reqs = {}
            for cat_obj in self.branch_map.values():
                cat_reqs = self._requires_cat_obj(cat_obj, merge_variables=True)
                for config_name, proc_reqs in cat_reqs.items():
                    hist_reqs.setdefault(config_name, {})
                    for proc_name, dataset_reqs in proc_reqs.items():
                        hist_reqs[config_name].setdefault(proc_name, {})
                        for dataset_name, task in dataset_reqs.items():
                            hist_reqs[config_name][proc_name].setdefault(dataset_name, set()).add(task)
            return reqs

        return reqs

    SerializeInferenceModelBase.workflow_requires = SerializeInferenceModelBase_workflow_requires

    def MergeShiftedHistograms_workflow_requires(self):
        # cannot call super() here; hopefully we only need the workflow_requires from RemoteWorkflow
        reqs = RemoteWorkflow.workflow_requires(self)

        if not self.pilot:
            # add nominal and both directions per shift source
            for shift in ["nominal"] + self.shifts:
                reqs[shift] = self.reqs.MergeHistograms.req(self, shift=shift, _prefer_cli={"variables"})
        else:
            # in pilot mode, only require the nominal histograms.
            # NOTE: would be nice to add all non-weight shifts instead of only nominal
            reqs["nominal"] = self.reqs.MergeHistograms.req(self, shift="nominal", _prefer_cli={"variables"})

        return reqs

    MergeShiftedHistograms.workflow_requires = MergeShiftedHistograms_workflow_requires
    logger.info(
        f"patched {MergeShiftedHistograms.task_family} to only require nominal histograms in pilot mode",
    )


@memoize
def patch_mltraining():
    from columnflow.tasks.framework.remote import RemoteWorkflow

    # patch the MLTraining output collection
    MLTraining.output_collection_cls = law.NestedSiblingFileCollection

    # remove unnceessary requires from MLTraining
    def workflow_requires(self):
        # cannot call super() here; hopefully we only need the workflow_requires from RemoteWorkflow
        reqs = RemoteWorkflow.workflow_requires(self)
        reqs["model"] = self.ml_model_inst.requires(self)
        return reqs

    def requires(self):
        reqs = {}
        reqs["model"] = self.ml_model_inst.requires(self)
        return reqs

    MLTraining.requires = requires
    MLTraining.workflow_requires = workflow_requires
    logger.info("patched MLTraining to use NestedSiblingFileCollection and remove unnecessary requires")


@memoize
def patch_column_alias_strategy():
    # NOTE: not used since checks always fail
    # patch the missing_column_alias_strategy for all tasks
    # at SelectEvents, the btag_weight alias is missing, therefore the check cannot be used
    SelectEvents.missing_column_alias_strategy = "raise"
    CreateCutflowHistograms.missing_column_alias_strategy = "raise"
    ReduceEvents.missing_column_alias_strategy = "raise"
    ProduceColumns.missing_column_alias_strategy = "raise"

    # I would like to add this tag, but since we need to request column aliases for JEC and cannot
    # apply aliases of the Jet.pt here,
    CreateHistograms.missing_column_alias_strategy = "raise"
    PrepareMLEvents.missing_column_alias_strategy = "raise"
    MLEvaluation.missing_column_alias_strategy = "raise"


@memoize
def patch_htcondor_workflow_naf_resources():
    """
    Patches the HTCondorWorkflow task to declare user-specific resources when running on the NAF.
    """
    from columnflow.tasks.framework.remote import HTCondorWorkflow

    def htcondor_job_resources(self, job_num, branches):
        # one "naf_<username>" resource per job, indendent of the number of branches in the job
        return {f"naf_{getpass.getuser()}": 1}

    HTCondorWorkflow.htcondor_job_resources = htcondor_job_resources

    logger.debug(f"patched htcondor_job_resources of {HTCondorWorkflow.task_family}")


@memoize
def patch_csp_versioning():
    """
    Patches the TaskArrayFunction to add the version to the string representation of the task.
    """

    from columnflow.tasks.framework.mixins import ArrayFunctionClassMixin

    def TaskArrayFunction_str(self):
        version = self.version() if callable(getattr(self, "version", None)) else getattr(self, "version", None)
        if version and not isinstance(version, (int, str)):
            raise Exception(f"version must be an integer or string, but is {version} ({type(version)})")
        version_str = f"V{version}" if version is not None else ""
        return f"{self.cls_name}{version_str}"

    def array_function_cls_repr(self, array_function):
        # NOTE: this might be a problem when we have identical names between different types of
        # TaskArrayFunctions...
        array_function_cls = TaskArrayFunction.get_cls(array_function)
        return TaskArrayFunction_str(array_function_cls)

    ArrayFunctionClassMixin.array_function_cls_repr = array_function_cls_repr
    TaskArrayFunction.__str__ = TaskArrayFunction_str
    logger.info(
        "patched TaskArrayFunction.__str__ to include the CSP version attribute",
    )

    from columnflow.tasks.framework.mixins import InferenceModelClassMixin
    from columnflow.inference import InferenceModel

    def InferenceModel_str(self: InferenceModel):
        version_str = f"V{self.version}" if getattr(self, "version", None) is not None else ""
        return f"{self.cls_name}{version_str}"

    @property
    def inference_model_repr(self: law.Task):
        """ Custom representation for InferenceModel """
        inference_model_cls = InferenceModel.get_cls(self.inference_model)

        version_str = (
            f"V{inference_model_cls.version}"
            if getattr(inference_model_cls, "version", None) is not None else ""
        )
        return f"{inference_model_cls.cls_name}{version_str}"
        # return str(inference_model_cls)

    InferenceModel.__str__ = InferenceModel_str
    InferenceModelClassMixin.inference_model_repr = inference_model_repr
    logger.info(
        "patched InferenceModelClassMixin.inference_model_repr to include the InferenceModel version attribute",
    )


@memoize
def patch_default_version():
    # setting the default version from the law.cfg
    default_version = law.config.get_expanded("analysis", "default_version", None)
    AnalysisTask.version = luigi.Parameter(
        default=default_version,
        description="mandatory version that is encoded into output paths",
    )
    logger.info(f"using default version '{default_version}' for all AnalysisTasks")


@memoize
def patch_materialization_strategy():
    """
    Simple patch function to switch to the PARTITIONS materialization strategy for DaskArrayReader.
    We might want to try in the future if this improves memory usage, but this requires us to
    reproduce all existing outputs with this type of partitioning.
    """
    from columnflow.columnar_util import DaskArrayReader

    # Save the original __init__ method
    _original_init = DaskArrayReader.__init__

    def patched_init(self, *args, **kwargs):
        logger.debug(f"patched DaskArrayReader.__init__ with {DaskArrayReader.MaterializationStrategy.PARTITIONS}")
        # Modify the materialization_strategy before calling the original __init__
        kwargs["materialization_strategy"] = (
            DaskArrayReader.MaterializationStrategy.PARTITIONS
        )
        _original_init(self, *args, **kwargs)

    # Replace the original __init__ with the patched version
    DaskArrayReader.__init__ = patched_init


@memoize
def patch_all():
    # change the "retries" parameter default
    from columnflow.tasks.framework.remote import RemoteWorkflow
    RemoteWorkflow.retries = RemoteWorkflow.retries.copy(default=3)

    patch_mltraining()
    patch_htcondor_workflow_naf_resources()
    # patch_column_alias_strategy()
    patch_csp_versioning()
    patch_default_version()
    patch_pilots()
    # NOTE: this patch is useful to make fetch-output consistent, but I am not yet sure if it breaks things
    # patch_live_task_id()
